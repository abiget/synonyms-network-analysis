{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dec661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/abiget/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import powerlaw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9968182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install powerlaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256d123",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12deeb3",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"SB7RLl13RiALifpPVLXfK3jAfXxiqAwO\"\n",
    "# svPU7ggOb1jfTScqf3F4f6CXjaPni13C\n",
    "\n",
    "# Base URL for Mistral's OpenAI-compatible chat completions endpoint\n",
    "API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "\n",
    "# Choose one of the available models (e.g., 'mistral-tiny', 'mistral-small', 'mistral-medium')\n",
    "MODEL = \"mistral-small\"\n",
    "\n",
    "# Headers for authentication and content type\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbca18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_mistral(messages):\n",
    "    \"\"\"\n",
    "    Sends a list of messages to the Mistral chat API and returns the assistant's response.\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): A list of message dictionaries in the OpenAI chat format.\n",
    "                         Example: [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "\n",
    "    Returns:\n",
    "        str: The assistant's reply as a string.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.7,     # Creativity level (0 = deterministic, 1 = more random)\n",
    "        \"top_p\": 1.0,           # Nucleus sampling parameter\n",
    "        \"stream\": False         # Disable streaming for simple usage\n",
    "    }\n",
    "\n",
    "    # Send a POST request to Mistral's API\n",
    "    response = requests.post(API_URL, headers=HEADERS, data=json.dumps(payload))\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to save the response to a file\n",
    "def save_response_to_file(response, filename=\"response.txt\"):\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(response + \"\\n\")\n",
    "    # print(f\"Response saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f28b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the common adjectives from a JSON file\n",
    "with open(\"data/common-adjectives.json\", \"r\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f92852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of common adjectives as base words\n",
    "print(len(data['CommonAdjectives']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83749b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(prompt):\n",
    "    # Initialize a conversation with a greeting\n",
    "    examples = [\n",
    "        \"happy → joyful,content,pleased\",\n",
    "        \"sad → unhappy,dejected,downcast\",\n",
    "        \"fast → quick,swift,rapid\",\n",
    "    ]\n",
    "    example = random.choice(examples)\n",
    "    \n",
    "    chat_history = [\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": (\n",
    "             f\"Generate only 3 to 10 synonyms for the word '{prompt}' as a single comma-separated string. \"\n",
    "             f\"Do not add explanations, greetings, or anything else — only return the synonyms in this exact format: word1,word2,word3. \"\n",
    "             f\"Example: {example}. No brackets, no quotes, no extra text, no whitespace between the first word and the second word except the comma. Just synonyms.\"\n",
    "         )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Send the message and receive the model's response\n",
    "        return chat_with_mistral(chat_history)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5002880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_with_retry(word, retries=3, delay=2):\n",
    "    for i in range(retries):\n",
    "        # Call the function to generate data\n",
    "        response = generate_data(word)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            return response_data['choices'][0]['message']['content']\n",
    "        elif response.status_code == 429:\n",
    "            wait_time = delay * (2 ** i)\n",
    "            print(f\"Rate limited on '{word}'. Waiting {wait_time}s...\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Error fetching synonyms for '{word}': {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e317fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store failed words when fetching synonyms\n",
    "failed_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words_from_base_words(base_words):\n",
    "    # Generate 3-10 synonyms for each word in the list\n",
    "    # base_words = data['CommonAdjectives']\n",
    "    # # base_words = extended_words\n",
    "    \n",
    "    for word in base_words:\n",
    "        prompt = f\"{word}\"\n",
    "        response = fetch_with_retry(prompt, retries=2, delay=2)\n",
    "        \n",
    "        if response is None:\n",
    "            print(f\"Failed to fetch synonyms for '{word}' after retries.\")\n",
    "            continue\n",
    "        # break\n",
    "        if re.match(r\"^[a-zA-Z-]+(,[a-zA-Z-]+){1,9}$\", response):\n",
    "            for synonym in response.split(\",\"):\n",
    "                save_response_to_file(word + \",\" + synonym.strip(), \"synonyms.csv\")\n",
    "            print(f\"{word}: {synonym.strip()}\")\n",
    "        else:\n",
    "            print(f\"Invalid response format for '{word}': {response}\")\n",
    "            failed_words.append(word)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synonyms for the common adjectives\n",
    "generate_words_from_base_words(data['CommonAdjectives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synonyms from the CSV file and remove duplicates\n",
    "synonyms_df = pd.read_csv(\"synonyms.csv\", header=None)\n",
    "synonyms_df.columns = [\"word\", \"synonym\"]\n",
    "synonyms_df = synonyms_df.drop_duplicates()\n",
    "\n",
    "unique_synonyms = synonyms_df['synonym'].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "unique_synonyms.to_csv(\"base_synonyms_extended.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of base words\n",
    "print(f\"Number of base words: {len(data['CommonAdjectives'])}\")\n",
    "# number of unique synonyms\n",
    "print(f\"Number of extended base words: {len(unique_synonyms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff69958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synonyms for the unique synonyms to expand our dataset\n",
    "generate_words_from_base_words(unique_synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5caee55",
   "metadata": {},
   "source": [
    "DATA Description:\n",
    "1. LLM\n",
    "\n",
    "We have used the most common 240 words from internet used them as a base word to generate 3-10 synonyms words using minstral llm and to extend our vocabulary we have used, 2373 unique words from the generated synonym words and agian generate 3-10 synonym words. \n",
    "- Number of base words: 240\n",
    "- Number of extended base words: 2373\n",
    "\n",
    "2. WordNet\n",
    "\n",
    "We have used all the unique base words (i.e 598) from the above process and try to get at most 10 synonym words from the wordnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016f86f",
   "metadata": {},
   "source": [
    "WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f189665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using WordNet to find synonyms\n",
    "synonyms = set()\n",
    "for syn in wn.synsets(\"hide\", ):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.add(lemma.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_synonyms_wordnet(word, pos=wn.ADJ, max_words=10):\n",
    "    \"\"\"\n",
    "    Create an edge list for a given word using WordNet.\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    # Get the synsets for the word\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                synonyms.add(lemma.name().strip().replace('_', '-').lower())\n",
    "                print(f\"{word}: {lemma.name().strip()}\")\n",
    "\n",
    "                if len(synonyms) >= max_words:\n",
    "                    # Save the synonyms to a CSV file\n",
    "                    for synonym in synonyms:\n",
    "                        save_response_to_file(word + \",\" + synonym.strip(), \"synonyms_wordnet.csv\")\n",
    "                        print(f\"{word}: {synonym.strip()}\")\n",
    "                    return\n",
    "\n",
    "    if synonyms is not None:\n",
    "        # If we reach here, the num of synonyms is less than max_words so we save them\n",
    "        for synonym in synonyms:\n",
    "            save_response_to_file(word + \",\" + synonym.strip(), \"synonyms_wordnet.csv\")\n",
    "            print(f\"{word}: {synonym.strip()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to process\n",
    "words_for_wordnet = pd.read_csv(\"synonyms.csv\", header=None, names=[\"word\", \"synonym\"])['word'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Generate synonyms for the unique words using WordNet\n",
    "words_for_wordnet.apply(lambda x: collect_synonyms_wordnet(x, pos=wn.ADJ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique words to be use as base words for WordNet:\", len(words_for_wordnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f65cb",
   "metadata": {},
   "source": [
    "Synonym Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0497cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonSynonymNetworkAnalyzer:\n",
    "    \"\"\"Analyze and compare common nodes between LLM and WordNet synonym networks\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_file_path, wordnet_file_path):\n",
    "        \"\"\"Initialize with paths to data files\"\"\"\n",
    "        self.llm_file_path = llm_file_path\n",
    "        self.wordnet_file_path = wordnet_file_path\n",
    "        \n",
    "        # Load the full networks\n",
    "        self.G_llm = self._load_network(llm_file_path)\n",
    "        self.G_wordnet = self._load_network(wordnet_file_path)\n",
    "        \n",
    "        # Extract common subgraphs\n",
    "        self._extract_common_subgraphs()\n",
    "        \n",
    "    def _load_network(self, file_path):\n",
    "        \"\"\"Load a network from a CSV file of word pairs\"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if not line or ',' not in line:\n",
    "                    continue\n",
    "                word1, word2 = line.split(',')\n",
    "                G.add_edge(word1, word2)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def _extract_common_subgraphs(self):\n",
    "        \"\"\"Extract and process common node subgraphs\"\"\"\n",
    "        # Find common nodes between networks\n",
    "        common_nodes = set(self.G_llm.nodes()).intersection(self.G_wordnet.nodes())\n",
    "        print(f\"Number of common nodes: {len(common_nodes)}\")\n",
    "        \n",
    "        # Create initial common subgraphs\n",
    "        self.G_llm_common = self.G_llm.subgraph(common_nodes).copy()\n",
    "        self.G_wordnet_common = self.G_wordnet.subgraph(common_nodes).copy()\n",
    "        \n",
    "        # Identify disconnected nodes\n",
    "        disconnected_llm = [n for n in self.G_llm_common.nodes if self.G_llm_common.degree(n) == 0]\n",
    "        disconnected_wordnet = [n for n in self.G_wordnet_common.nodes if self.G_wordnet_common.degree(n) == 0]\n",
    "        \n",
    "        print(f\"Disconnected nodes in LLM: {len(disconnected_llm)}\")\n",
    "        print(f\"Disconnected nodes in WordNet: {len(disconnected_wordnet)}\")\n",
    "        \n",
    "        # Create connected subgraphs by removing disconnected nodes\n",
    "        connected_nodes_llm = [n for n in self.G_llm_common.nodes if n not in disconnected_llm]\n",
    "        connected_nodes_wordnet = [n for n in self.G_wordnet_common.nodes if n not in disconnected_wordnet]\n",
    "        \n",
    "        self.G_llm_common_connected = self.G_llm_common.subgraph(connected_nodes_llm).copy()\n",
    "        self.G_wordnet_common_connected = self.G_wordnet_common.subgraph(connected_nodes_wordnet).copy()\n",
    "\n",
    "    def benchmark_random_graphs(self, G, num_simulations=100, swaps_per_edge=5):\n",
    "        \"\"\"Benchmark random graphs using double edge swap method\"\"\"\n",
    "        nswap = swaps_per_edge * G.number_of_edges()\n",
    "        num_tries = 5 * nswap\n",
    "\n",
    "        L_list = []\n",
    "        C_list = []\n",
    "\n",
    "        for _ in range(num_simulations):\n",
    "            G_rand = nx.double_edge_swap(G.copy(), nswap=nswap, max_tries=num_tries)\n",
    "\n",
    "            C = nx.average_clustering(G_rand)\n",
    "\n",
    "            largest_cc = max(nx.connected_components(G_rand), key=len)\n",
    "            G_sub = G_rand.subgraph(largest_cc).copy()\n",
    "            L = nx.average_shortest_path_length(G_sub)\n",
    "\n",
    "            L_list.append(L)\n",
    "            C_list.append(C)\n",
    "\n",
    "        Lrandom = np.mean(L_list)\n",
    "        Crandom = np.mean(C_list)\n",
    "\n",
    "        return Crandom, Lrandom\n",
    "    \n",
    "\n",
    "    def _plot_degree_distributions(self, fit_wnet, fit_llm, names):\n",
    "        \"\"\"Plot degree distributions of common node networks\"\"\"\n",
    "        # Plot the degree distribution\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "        \n",
    "        fig.supxlabel(\"Degree\")\n",
    "        fig.supylabel(\"Probability\")\n",
    "        fig.suptitle(\"Degree Distribution of WordNet and LLM Networks\", fontsize=14)\n",
    "\n",
    "        fit_wnet.plot_pdf(ax=axes[0], color='b', label='Degree Distribution', marker='o', markersize=6)\n",
    "        fit_wnet.power_law.plot_pdf(ax=axes[0], color='r', label='Power law fit', marker='o', markersize=6)\n",
    "        fit_wnet.exponential.plot_pdf(ax=axes[0], color='g', label=\"Exponential fit\", marker='o', markersize=6)\n",
    "        # axes[0].set_title(f\"Degree Distribution of {names[0]} Network\")\n",
    "        # axes[0].set_xlabel(\"Degree\")\n",
    "        # axes[0].set_ylabel(\"Probability\")\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        fit_llm.plot_pdf(ax=axes[1], color='b', label='Degree Distribution', marker='o', markersize=6)\n",
    "        fit_llm.power_law.plot_pdf(ax=axes[1], color='r', label='Power law fit', marker='o', markersize=6)\n",
    "        fit_llm.exponential.plot_pdf(ax=axes[1], color='g', label=\"Exponential fit\", marker='o', markersize=6)\n",
    "        # axes[1].set_title(f\"Degree Distribution of {names[1]} Network\")\n",
    "        # axes[1].set_xlabel(\"Degree\")\n",
    "        # axes[1].set_ylabel(\"\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/{names[0]}_{names[1]}_degree_distribution.eps', dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    def check_for_power_law(self, G_wnet, G_llm, names):\n",
    "        \"\"\"Check if the degree distribution follows a power law\"\"\"\n",
    "\n",
    "        degree_sequence_wnet = sorted([d for n, d in G_wnet.degree()], reverse=True)\n",
    "        degree_sequence_llm = sorted([d for n, d in G_llm.degree()], reverse=True)\n",
    "\n",
    "        # Fit distributions\n",
    "        fit_wnet = powerlaw.Fit(degree_sequence_wnet, discrete=True)\n",
    "        fit_llm = powerlaw.Fit(degree_sequence_llm, discrete=True)\n",
    "\n",
    "         # compare the data distribution fit power law vs exponential for Wordnet\n",
    "        R_wnet, p_wnet = fit_wnet.distribution_compare('power_law', 'exponential')\n",
    "        print(f\"{names[0]}--------------------------\")\n",
    "        print(f\"Power-law vs Exponential: R = {R_wnet}, p = {p_wnet} => {'sigificant with 5% level' if p_wnet < 0.05 else 'not significant'}\")\n",
    "\n",
    "        # compare the data distribution fit power law vs exponential for LLM\n",
    "        R_llm, p_llm = fit_llm.distribution_compare('power_law', 'exponential')\n",
    "        print(f\"{names[1]}--------------------------\")\n",
    "        print(f\"Power-law vs Exponential: R = {R_llm}, p = {p_llm} => {'significatn with 5% level' if p_llm < 0.05 else 'not significant'}\")\n",
    "\n",
    "        # plot degree distributions along with the fitted distributions\n",
    "        self._plot_degree_distributions(fit_wnet, fit_llm, names)\n",
    "\n",
    "    \n",
    "    def compare_networks(self):\n",
    "        \"\"\"Compare basic properties of common node networks\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for name, G in [(\"WordNet\", self.G_wordnet_common_connected), \n",
    "                        (\"LLM\", self.G_llm_common_connected)]:\n",
    "            num_nodes = G.number_of_nodes()\n",
    "            num_edges = G.number_of_edges()\n",
    "            density = nx.density(G)\n",
    "            avg_clustering = nx.average_clustering(G)\n",
    "            # calculate closeness centrality\n",
    "            avg_closeness_centrality = sum(nx.closeness_centrality(G).values())/num_nodes\n",
    "            # calculate betweenness centrality\n",
    "            avg_betweenness_centrality = sum(nx.betweenness_centrality(G).values())/num_nodes\n",
    "            # calculate degree centrality\n",
    "            avg_degree_centrality = sum(nx.degree_centrality(G).values())/num_nodes\n",
    "\n",
    "            components = list(nx.connected_components(G))\n",
    "            largest_cc = max(components, key=len)\n",
    "            G_lcc = G.subgraph(largest_cc).copy()\n",
    "\n",
    "            # the number of nodes in the largest connected component\n",
    "            num_nodes_lcc = G_lcc.number_of_nodes()\n",
    "\n",
    "            # calculate average shortest path length\n",
    "            avg_shortest_path_length = nx.average_shortest_path_length(G_lcc)\n",
    "\n",
    "            # Calculate average clustering coefficient for the largest connected component of random graphs\n",
    "            clustering_coeff_random, shortest_path_length_random = self.benchmark_random_graphs(G)\n",
    "            \n",
    "            metrics[name] = {\n",
    "                \"nodes\": num_nodes,\n",
    "                \"edges\": num_edges,\n",
    "                \"density\": density,\n",
    "                \"closeness_centrality\": avg_closeness_centrality,\n",
    "                \"betweenness_centrality\": avg_betweenness_centrality,\n",
    "                \"degree_centrality\": avg_degree_centrality,\n",
    "                \"number_of_components\": len(components),\n",
    "                \"nodes_in_largest_cc\": num_nodes_lcc,\n",
    "                \"avg_clustering\": avg_clustering,\n",
    "                \"avg_shortest_path_length_lcc\": avg_shortest_path_length,\n",
    "                \"avg_clustering_random\": clustering_coeff_random,\n",
    "                \"avg_shortest_path_length_random_lcc\": shortest_path_length_random\n",
    "            }\n",
    "\n",
    "        # Check for power law distribution\n",
    "        self.check_for_power_law(self.G_wordnet_common_connected, self.G_llm_common_connected, [\"WordNet\", \"LLM\"])\n",
    "        \n",
    "        return pd.DataFrame(metrics)\n",
    "    \n",
    "    def get_top_nodes_by_degree(self, k=10):\n",
    "        \"\"\"Get top nodes by degree for both common networks\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, G in [(\"WordNet\", self.G_wordnet_common_connected), \n",
    "                        (\"LLM\", self.G_llm_common_connected)]:\n",
    "            degree_dict = dict(G.degree())\n",
    "            sorted_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "            results[name] = sorted_nodes\n",
    "            \n",
    "            print(f\"Top {k} nodes by degree in {name} Common Network:\")\n",
    "            for node, degree in sorted_nodes:\n",
    "                print(f\"  {node}: {degree}\")\n",
    "            print(\"=\"*40)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_top_clustering_nodes(self, k=10):\n",
    "        \"\"\"Get top nodes by clustering coefficient for both common networks\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, G in [(\"WordNet\", self.G_wordnet_common_connected), \n",
    "                        (\"LLM\", self.G_llm_common_connected)]:\n",
    "            clustering_dict = nx.clustering(G)\n",
    "            sorted_nodes = sorted(clustering_dict.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "            results[name] = sorted_nodes\n",
    "            \n",
    "            print(f\"Top {k} nodes by clustering in {name} Common Network:\")\n",
    "            for node, coeff in sorted_nodes:\n",
    "                print(f\"  {node}: {coeff:.4f}\")\n",
    "            print(\"=\"*40)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_neighborhood(self, word, radius=2, layout='spring'):\n",
    "        \"\"\"Visualize neighborhood of a word in both common networks\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "        for (label, G), axis in zip([(\"LLM\", self.G_llm_common_connected), \n",
    "                        (\"WordNet\", self.G_wordnet_common_connected)], axes):\n",
    "            if word not in G:\n",
    "                print(f'The word \"{word}\" is not in the {label} graph.')\n",
    "                continue\n",
    "            \n",
    "            self._plot_neighborhood(G, word, radius, label, layout, axis)\n",
    "        plt.suptitle(f\"Neighborhood of '{word}' (radius={radius}) LLM vs WordNet\", fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figures/{word}_neighborhood_comparison.eps', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _plot_neighborhood(self, G, word, radius=2, label=None, layout='spring', axis=None):\n",
    "        \"\"\"Helper method to plot neighborhood of a word\"\"\"\n",
    "        # Create ego graph\n",
    "        neighborhood = nx.ego_graph(G, word, radius=radius)\n",
    "\n",
    "        # Use Seaborn's whitegrid style for a cleaner aesthetic\n",
    "        sns.set_style(\"whitegrid\")\n",
    "\n",
    "        # Choose layout\n",
    "        if layout == 'spring':\n",
    "            pos = nx.spring_layout(neighborhood, seed=42)\n",
    "        elif layout == 'circular':\n",
    "            pos = nx.circular_layout(neighborhood)\n",
    "        elif layout == 'kamada_kawai':\n",
    "            pos = nx.kamada_kawai_layout(neighborhood)\n",
    "        elif layout == 'spectral':\n",
    "            pos = nx.spectral_layout(neighborhood)\n",
    "        else:\n",
    "            pos = nx.spring_layout(neighborhood, seed=42)\n",
    "\n",
    "        # Node and edge drawing\n",
    "        nx.draw_networkx_edges(\n",
    "            neighborhood, pos, connectionstyle='arc3, rad=0.2',\n",
    "            edge_color='gray', alpha=0.5, arrows=True, ax=axis\n",
    "        )\n",
    "\n",
    "        nx.draw_networkx_nodes(\n",
    "            neighborhood, pos, node_color='#69b3a2',\n",
    "            node_size=0, edgecolors='black', linewidths=0.6, ax=axis\n",
    "        )\n",
    "\n",
    "        nx.draw_networkx_labels(\n",
    "            neighborhood, pos, font_size=10,\n",
    "            font_weight='bold', font_color='black',\n",
    "            ax=axis\n",
    "        )\n",
    "\n",
    "        axis.axis('off')\n",
    "        # axis.set_title(\n",
    "        #     f\"Synonyms Network of '{word}' (radius={radius})\" + (f\" - {label}\" if label else \"\"),\n",
    "        #     fontsize=16, fontweight='bold'\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67059348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common nodes: 939\n",
      "Disconnected nodes in LLM: 23\n",
      "Disconnected nodes in WordNet: 66\n"
     ]
    }
   ],
   "source": [
    "analyzer = CommonSynonymNetworkAnalyzer(\n",
    "    llm_file_path='data/synonyms.csv',\n",
    "    wordnet_file_path='data/synonyms_wordnet.csv'\n",
    ")\n",
    "\n",
    "# Analyze the common networks\n",
    "analyzer.compare_networks()\n",
    "analyzer.get_top_nodes_by_degree()\n",
    "\n",
    "# Visualize specific words\n",
    "for word in ['happy', 'glum']:\n",
    "    analyzer.visualize_neighborhood(word, layout='spring')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2fe40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
